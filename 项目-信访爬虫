# 目的：
从天涯论坛，新浪微博爬取关于上访的内容
完成时间：
0113 周六
考核：
跑出想要的结果，梦圆认同
=
# 计划:
0102-0106 熟悉爬虫，并且实现爬虫抓取天涯网页内容
0103 周三 跟着案例学爬虫并用Python实现它
0104 周四 更高级的爬虫案例并且实现
0105 周五 从天涯爬取内容；和嘉敏确定爬虫字段，并且统一格式；解决翻页的问题
0106 周六 实战，并且用更简洁的代码；再次尝试翻页的rule及其他方法
0106 周六 临时调整工作 协助怡婷整理文档
0108-0113 正式开始爬虫
0108 周一 实战爬取一级页面，并且用更简洁的代码；再次尝试翻页的rule及其他方法
0109 周二 复盘爬取经验；实现二级页面的爬取
0110 周三 天涯多页面二级页面的爬取。爬虫经验的复盘；爬一个页面
0111 周四 从实际出发，爬取二级页面，爬取最终页面
0112 周五 爬取多页 二级页面 最终页面
===

# 爬虫：
定义：
网页是html文档，通过HTTP协议，客户机向服务器发送请求，获得html文档
爬虫的目的是获得从html文档，并从文档中挑选自己想要的东西
-
request对象和response对象。客户机与服务器交换数据时用到，由服务器创立
http://blog.csdn.net/a859522265/article/details/7259115
=
# 安装anaconda
是Python数据分析的发行版本，集成大量第三方的包，再也不用担心安装包的问题了 https://www.jianshu.com/p/169403f7e40c
打开jupyter： anaconda prompt-jupyter notebook
更改jupyter路径： anaconda - cd c:\on -jupyter notebook
安装包：
1 anaconda prompt-anaconda install package_name
2 anaconda navigator
包只会安装最新的
===

# scrapy
Scrapy是一个为了爬取网站数据和提取结构性数据而编写的应用框架，可以省去造轮子的时间
Python爬虫进阶一之爬虫框架概述： 
https://cuiqingcai.com/2433.html
http://aljun.me/post/4
入门教程： 
https://doc.scrapy.org/en/1.5/topics/commands.html
工作流程：
获取url-发送request-接收response-使用spider解析-发送到pipelines存储成想要的形式
-
主文件的作用：
spider文件 爬虫文件，解析response的html和xml
item 用来储存要爬下来的数据的存放容器，类似orm的写法
settings.py 系统设置，主要是反扒操作
pipeline 执行保存数据的操作
-
Scrapy报错：
unknown command : crawl :指定到文件夹路径
403。网站屏蔽了IP
-
经验：
Scrapy中的Rules理解 http://blog.csdn.net/wqh_jingsong/article/details/56865433
=
# 从案例学习scrapy
用rule爬取到2级页面 http://blog.csdn.net/u010814042/article/details/74127309
使用for方法爬取多页 http://blog.csdn.net/Gavin_CHEN929/article/details/56509735
爬取多页 http://blog.csdn.net/konglei1996/article/details/72511632
爬取多页2 request http://blog.csdn.net/dylanzr/article/details/51764694
-
问题：
class Student(object)，object类，这是所有类最终都会继承的类
文件读取与写入 http://dwz.cn/7bnhGl
read http://www.runoob.com/python/python-file-read.html
yield 变成迭代器 http://dwz.cn/6CFZJh
-
经验：
爬取的时间较长：因为设置了等待时间，以防服务器认定为爬虫
def process_item(self, item, spider): item, spider应该是上面已经自动生成的
===

# 天涯论坛
http://search.tianya.cn/
http://search.tianya.cn/bbs?q=%E4%B8%8A%E8%AE%BF&pn=1
爬虫的原因：
掌握网上信访人信息和动态，近半年的动态
流程：
确定爬取页-确定爬取的字段-数据清洗-采用关联表格，整理到数据库
数据探索：
发帖者只是为了反映自己的情况，帖子数一般是1个左右，回帖不会超过10
最多通过自己的发帖回帖
-
问题：
输出文件隔行为空
有些结果就是爬不下来
限制匹配个数 如只能匹配3个字符
多级页面爬取 如何匹配id和内容
用for方法的爬取顺序乱
-
经验：
每运行完一个spider文件，吃透它
从返回命令中找失败的原因
从成功中总结经验，并且不断测试，形成固定的方法论
从项目入手，测试需要的结果
注重效率，不用怕犯错
八爪鱼试试

获得xpath 审查元素-copy-copy xpath，修改xpath代码的div即可
复制文本，再点击审查元素会更快获得xpath路径
不要全盘复制黏贴，因为本身有固定的代码
must be real number, not str: 一般是正则表达的问题
''' 与下行的缩进保持一致才不会报错
rule 用于爬取二级页面
CrawSpider中rules中的使用 https://www.cnblogs.com/zhangjpn/p/6802583.html
Selector和response的用法没有差别，可互相替换
用for语句存储item才能保证爬取的字符在Excel中单独显示
xpath不是万能的，对于某些代码是没办法解析的
将yield放入parse会多出starturl结果
回调函数  把主函数的参数传入另一函数处理 https://en.wikipedia.org/wiki/Callback_(computer_programming)#Python
爬虫并不是按照顺序爬取
500 Servlet Exception 直接输入链接可以解决

Python正则表达式 http://www.runoob.com/python/python-reg-expressions.html
反斜杠转义\
yield 只能在function里面使用
-
http://wiki.mbalib.com/wiki/%E9%A6%96%E9%A1%B5 是可以用rule爬到4级页面的
不同的项目文件，结果竟然会不一样。可能原因是网址变了
Python爬虫|深入请求（四）常见的反爬机制以及应对方法 https://zhuanlan.zhihu.com/p/21558661
timesleep 随机休眠


