# 目的：
从天涯论坛，新浪微博爬取关于上访的内容
完成时间：
0113 周六
考核：
跑出想要的结果，梦圆认同
=
# 计划:
0102-0106 熟悉爬虫，并且实现爬虫抓取天涯网页内容
0103 周三 跟着案例学爬虫并用Python实现它
0104 周四 更高级的爬虫案例并且实现
0105 周五 从天涯爬取内容；和嘉敏确定爬虫字段，并且统一格式；解决翻页的问题
0106 周六 从天涯爬取内容；解决二级页面爬取的问题
0108-0113 正式开始爬虫
===

# 爬虫：
定义：
网页是html文档，通过HTTP协议，客户机向服务器发送请求，获得html文档
爬虫的目的是获得从html文档，并从文档中挑选自己想要的东西
-
流程：
获取数据-从数据中获取想要的数据-存储
=
# 爬虫学习文档
python3 爬虫之路 http://blog.csdn.net/column/details/python3-spider.html
Python 3 网络爬虫学习建议 https://www.zhihu.com/question/41277528/answer/96409506
Python爬虫学习系列教程 https://cuiqingcai.com/1052.html
Python爬虫|深入请求（四）常见的反爬机制以及应对方法 https://zhuanlan.zhihu.com/p/21558661
网易云课堂
Scrapy https://www.jianshu.com/p/e71ff6173302
=
# 安装anaconda
是Python数据分析的发行版本，集成大量第三方的包，再也不用担心安装包的问题了 https://www.jianshu.com/p/169403f7e40c
打开jupyter： anaconda prompt-jupyter notebook
更改jupyter路径： anaconda - cd c:\on -jupyter notebook
安装包：
1 anaconda prompt-anaconda install package_name
2 anaconda navigator
包只会安装最新的
=
# 使用爬虫会遇到的困难：
需要模拟浏览器方式访问
需要登录
需要cookie
需要动态验证码
跑不动，对方通过行为判定爬虫行为
=
# 使用爬虫会用到的知识
正则表达式
scrapy的框架
request对象和response对象:
http://blog.csdn.net/a859522265/article/details/7259115
客户机与服务器交换数据时用到，由服务器创立
===

# scrapy
Python爬虫进阶一之爬虫框架概述： 
https://cuiqingcai.com/2433.html
http://aljun.me/post/4
Scrapy是一个为了爬取网站数据和提取结构性数据而编写的应用框架，可以省去造轮子的时间
入门教程： 
https://doc.scrapy.org/en/1.5/topics/commands.html
http://scrapy-chs.readthedocs.io/zh_CN/latest/intro/tutorial.html
工作流程：
获取url-发送request-接收response-使用spider解析-发送到pipelines存储成想要的形式
-
主文件的作用：
item 用来储存要爬下来的数据的存放容器，类似orm的写法
spider文件 爬虫文件，解析response的html和xml
pipeline 执行保存数据的操作
settings.py 系统设置，主要是反扒操作
获得response-解析response-存储到item
-
出现的错误：
unknown command : crawl :指定到文件夹路径
scrapy 输出的文件为空。网站设置了路障，或者根本没爬到内容
spider not found：spider文件名 name
=
# 实际案例学习：
问题：
? 二进制格式转换为str格式
-
爬取豆瓣电影 
1 
scrapy爬取豆瓣TOP250电影（转成csv） http://blog.csdn.net/Gavin_CHEN929/article/details/56509735
爬取多个页面的另一个方法 http://blog.csdn.net/dylanzr/article/details/51764694
爬取单个页面 http://blog.csdn.net/konglei1996/article/details/72511632
问题：
爬取到的结果少了排名201-225的数据
第二次爬出来的重复
2
用scrapy对豆瓣top250页面爬取 
http://aljun.me/post/5
http://aljun.me/post/4
问题：
crawled 0 pages 没有采集到内容
3 http://blog.csdn.net/u010814042/article/details/74127309
问题：
返回的是二进制的文件:转编码格式就行了
爬取的时间较长：因为设置了等待时间，以防服务器认定为爬虫
不懂翻页的方法
-
需要学习的知识：
文件读取与写入 http://dwz.cn/7bnhGl
read http://www.runoob.com/python/python-file-read.html
类：
class Student(object)
object类，这是所有类最终都会继承的类
yield 变成迭代器 http://dwz.cn/6CFZJh
问题：
? def process_item(self, item, spider): item, spider应该是上面已经自动生成的
===

# 天涯论坛
http://search.tianya.cn/
流程：
确定爬取页-确定爬取的字段-采用关联表格，整理到数据库
爬虫的原因：
掌握网上信访人信息和动态
技术实现战略：
实现难度
-
问题：
无法用Rule实现所有页面的爬取，它代替了正则表达式
输出文件隔行为空
经验：
获得xpath 审查元素-copy-copy xpath，修改xpath代码的div即可
不要全盘复制黏贴，因为本身有固定的代码
must be real number, not str: 
-


