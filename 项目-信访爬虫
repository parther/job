# 目的：
从天涯论坛，新浪微博爬取关于上访的内容
完成时间：
0113 周六
考核：
跑出想要的结果，梦圆认同
=
# 计划:
0102-0106 熟悉爬虫，并且实现爬虫抓取天涯网页内容
0103 周三 跟着案例学爬虫并用Python实现它
0104 周四 更高级的爬虫案例并且实现
0105 周五 从天涯爬取内容；和嘉敏确定爬虫字段，并且统一格式；解决翻页的问题
0106 周六 实战，并且用更简洁的代码；再次尝试翻页的rule及其他方法
0106 周六 临时调整工作 协助怡婷整理文档
0108-0113 正式开始爬虫
0108 周一 实战爬取一级页面，并且用更简洁的代码；再次尝试翻页的rule及其他方法
0109 周二 复盘爬取经验；实现二级页面的爬取
0110 周三 天涯多页面二级页面的爬取。爬虫经验的复盘；爬一个页面
0111 周四 从实际出发，爬取二级页面，爬取最终页面
0112 周五 实现天涯单页多级页面的爬取v；爬取最终页的内容和评论v；下午4点跟梦圆讨论下
0113 周六 从实际出发，各级别页面小规模测试；大规模页面爬取，使用反扒措施；解决csv隔行等问题
0115-0120 大规模爬取
0115 周一 设置代理反爬
===

# 爬虫：
定义：
网页是html文档，通过HTTP协议，客户机向服务器发送请求，获得html文档
爬虫的目的是获得从html文档，并从文档中挑选自己想要的东西
-
request对象和response对象。客户机与服务器交换数据时用到，由服务器创立
http://blog.csdn.net/a859522265/article/details/7259115
=
# 安装anaconda
是Python数据分析的发行版本，集成大量第三方的包，再也不用担心安装包的问题了 https://www.jianshu.com/p/169403f7e40c
打开jupyter： anaconda prompt-jupyter notebook
更改jupyter路径： anaconda - cd c:\on -jupyter notebook
安装包：
1 anaconda prompt-anaconda install package_name
2 anaconda navigator
包只会安装最新的
===

# scrapy
Scrapy是一个为了爬取网站数据和提取结构性数据而编写的应用框架，可以省去造轮子的时间
Python爬虫进阶一之爬虫框架概述： 
https://cuiqingcai.com/2433.html
http://aljun.me/post/4
入门教程： 
https://doc.scrapy.org/en/1.5/topics/commands.html
工作流程：
获取url-发送request-接收response-使用spider解析-发送到pipelines存储成想要的形式
-
主文件的作用：
spider文件 爬虫文件，解析response的html和xml
item 用来储存要爬下来的数据的存放容器，类似orm的写法
settings.py 系统设置，主要是反扒操作
pipeline 执行保存数据的操作
-
Scrapy报错：
unknown command : crawl :指定到文件夹路径
403。网站屏蔽了IP
-
经验：
Scrapy中的Rules理解 http://blog.csdn.net/wqh_jingsong/article/details/56865433
=
# 从案例学习scrapy
用rule爬取到2级页面 http://blog.csdn.net/u010814042/article/details/74127309
使用for方法爬取多页 http://blog.csdn.net/Gavin_CHEN929/article/details/56509735
爬取多页 http://blog.csdn.net/konglei1996/article/details/72511632
爬取多页2 request http://blog.csdn.net/dylanzr/article/details/51764694
-
问题：
class Student(object)，object类，这是所有类最终都会继承的类
文件读取与写入 http://dwz.cn/7bnhGl
read http://www.runoob.com/python/python-file-read.html
yield 变成迭代器 http://dwz.cn/6CFZJh
-
经验：
爬取的时间较长：因为设置了等待时间，以防服务器认定为爬虫
def process_item(self, item, spider): item, spider应该是上面已经自动生成的
===

# 爬取天涯论坛
http://search.tianya.cn/
http://search.tianya.cn/bbs?q=%E4%B8%8A%E8%AE%BF&pn=1
爬虫的原因：
掌握网上信访人信息和动态，近半年的动态
流程：
确定爬取页-确定爬取的字段-数据清洗-采用关联表格，整理到数据库
---
数据探索：
发帖者只是为了反映自己的情况，帖子数一般是1个左右，回帖不会超过10
最多通过自己的发帖回帖
图片是少数
一般针对评论的回复比较少，上图的也少
猜测 昵称会变，但是个人主页uid是不会变
针对主贴评论人分析昵称所指的人，但有可能是专门帮忙发帖的
天涯盖楼最多可以盖到100楼
能够从文章的内容中提取出上访人的身份信息 上访人一般实名举报
---
表结构：
1 姓名-基本信息 info
2 姓名-帖子标题及内容 post
3 姓名-帖子标题及内容-评论及姓名 comment
4 姓名-帖子标题及内容-评论及姓名-回复
5 姓名-回帖内容
-
表结构抓取内容：
1 姓名-基本信息 info
昵称
关注个数 
粉丝个数 说明影响力
积分 说明活跃度
注册日期 说明上访事件发生的节点
生日
地区
行业
职业
标签
个人介绍
主贴数 说明活跃度
回帖数 说明活跃度
3及页面-关注和粉丝：
4级页面-主贴页需要抓取的内容：
标题v
姓名v
时间v
点击v
回复v
主贴内容v
回帖人v
回帖时间v
回帖内容v
针对回帖的打赏、评论、回复、赞
-
页面处理的层次：
单页/单页多内容
单页多级
多页/多页多内容
多页多级
---
问题：
输出文件隔行为空
http://blog.csdn.net/kangqianglong/article/details/53173575
https://segmentfault.com/q/1010000010632474
限制匹配个数 如只能匹配3个字符
爬下来字段的顺序是乱的 https://www.jianshu.com/p/fd6f7eba6abe
文章中有图片该如何处理
抽取链接后的数字如 http://www.tianya.cn/130935919
用for方法翻页无法实现自动化采集，但是可以作为权宜之计
分布式 提高爬虫的效率
-
经验：
每运行完一个spider文件，吃透它
从返回命令中找失败的原因
从成功中总结经验，并且不断测试，形成固定的方法论
从项目入手，测试需要的结果
注重效率，不用怕犯错
使用现成的爬虫工具如八爪鱼
设置notepad 消灭空格 技术产生进步
有些结果就是爬不下来
不要全盘复制黏贴，因为本身有固定的代码
-
获得xpath 审查元素-copy-copy xpath，修改xpath代码的div即可
复制文本，再点击审查元素会更快获得xpath路径
寻找xpath，可以将多个要爬取的xpath拿出来比较而得出规律
'//div[@class="bbs-content"]/text()' xpath的另一种方式
xpath不是万能的，对于某些代码是没办法解析的
rule 用于爬取多级页面
不同的项目文件，结果竟然会不一样。可能原因是网址变了
http://wiki.mbalib.com/wiki/%E9%A6%96%E9%A1%B5 是可以用rule爬到4级页面的
CrawSpider中rules中的使用 https://www.cnblogs.com/zhangjpn/p/6802583.html
Selector和response的用法没有差别，可互相替换
用for语句存储item才能保证爬取的字符在Excel中单独显示
将yield放入parse会多出starturl结果
回调函数  把主函数的参数传入另一函数处理 https://en.wikipedia.org/wiki/Callback_(computer_programming)#Python
500 Servlet Exception 直接输入链接可以解决
response.xpath 返回的是list数据
scrapy crawl 不能直接跟py文件
存储路径的两种写法：
FEED_URI = u'file:///C:/Users/Administrator/Desktop/douban.csv'
FEED_URI = './xf_info.csv'
-
Python正则表达式 http://www.runoob.com/python/python-reg-expressions.html
反斜杠转义\
yield 只能在function里面使用
if not a ;if a=='' ;if a 判断为空
list out of range 条目不对
must be real number, not str: 一般是正则表达的问题
''' 与下行的缩进保持一致才不会报错
---

Python爬虫|深入请求（四）常见的反爬机制以及应对方法 https://zhuanlan.zhihu.com/p/21558661
timesleep 随机休眠
IP 代理池 http://blog.csdn.net/u011781521/article/details/70194744?locationNum=4&fps=1
利用crawlera神器，无需再寻找代理IP http://blog.csdn.net/xiao4816/article/details/50650075

1 js动态页面加载，内容在源码里是看不到的
post_number = response.xpath('//div[@class="mod-hd"]/span/a[1]/text()').extract()
火狐firebug可看动态加载的内容
Python爬虫利器五之Selenium的用法 https://cuiqingcai.com/2599.html
Selenium beautifulsoup的用法 http://cuiqingcai.com/1052.html
2 数据能读取到，但是xpath没有规律 信息列的。直接全部爬取下来
https://www.jianshu.com/p/4e6e42945f05
http://www.tianya.cn/134503625
http://www.tianya.cn/113953041


