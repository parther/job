# 为什么要看这本书：
我想转行数据挖掘，这本书是数据挖掘的入门书籍
看这本书想达到的实际目的：
4 每一个模型能够用Python代码跑出来
3 手动推导出来
2 实际应用的场景
1 模型的实现流程，背后的原理
李航 统计学习方法
周志华 机器学习
机器学习实战
Python数据挖掘入门与实践


===
# 问题
1 argmaxP(y|x)： 取p(y|x)最大时x的取值     https://baike.baidu.com/item/argmax/6034072
2 极大似然估计： 
相当于argmaxp(x|y)，求使p(x|y)最大时y的取值
已知某个参数y能够使事件x出现的概率最大，如果一次试验中事件x发生了，此时y的值应该是使x发生概率最大的那一个
https://baike.baidu.com/item/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/3350286
3 为什么感知机模型 w是分离超平面的法向量：
4 理解超平面 http://blog.csdn.net/denghecsdn/article/details/77313758
5 泛化能力的误差上界
6 感知机算法的收敛性
7 感知机模型的对偶形式
8 k近邻法的误分类率
# 知识点
1  膜长
指向量的长度 ||v||
√v*v 在空间中，可以通过勾股定理求出
2 内积也是点积
3 ε 非常小的正数
4 联合概率分布：p(xny)，p(x,y)
先验概率，p(x)
后验概率, p(x|y)
贝叶斯定理 https://zhuanlan.zhihu.com/p/22467549
5 法向量:向量点积为0
6 L2范数 ||x||：
向量中各元素的平方和开方，相当于求距离，常用于欧式距离
范数简介 https://wenku.baidu.com/view/d9231bf089eb172ded63b744.html
几种范数介绍 http://blog.csdn.net/shijing_0214/article/details/51757564
===
# 统计学习方法：
是什么：
输入训练数据，训练计算机学习概率统计模型，利用学得的最优模型对数据进行分析和预测的方法
学习：
通过数据和模型提高计算机分析和预测的能力
统计学习的前提：
同类数据具有一定的统计规律性, 数据独立同分布
学习的方式：
1 监督学习
2 非监督学习 
3 半监督学习 http://blog.csdn.net/mousever/article/details/51526540
4 强化学习 https://segmentfault.com/a/1190000007813298

学习的步骤：
输入训练数据 - 确定假设空间 - 确定选择模型的准则 - 通过算法实现最优模型 - 预测
统计学习三要素：
1 模型：假设数据独立同分布产生的，假设要学习的模型属于某类函数的集合（假设空间）
2 策略：确立选择模型的准则，从假设空间中选择最优模型
3 方法：通过算法求出最优模型
=
# 监督学习
输入空间，特征空间，输出空间：
特征空间：特征值是输入值X,X的集合被称为特征空间，X是特征空间的随机变量取值。特征空间属于输入空间
输出空间：输出空间是输出值Y的集合，Y是输出空间的随机变量取值

问题归类：
1 输入值和输出值为连续变量的预测问题称为回归问题
2 输入值为离散变量的预测问题称为分类问题
3 输入值和输出变量为变量序列的预测问题称为标注问题
标注问题是分类问题的推广，与分类相比输入的是向量。自然语言处理中运用最多
分类、标注与回归 http://blog.csdn.net/losteng/article/details/51027131
词性标注 http://blog.csdn.net/xyls12345/article/details/25977741

监督学习的前提：
1 输入空间随机变量X和输出空间随机变量Y 由 联合概率分布P(X,Y)生成
2 训练数据和测试数据 独立同分布产生的
监督学习目的：
经过训练找到的最优模型，使预测值与实际值的差值最小
监督学习的结果：
模型属于输入到输出空间映射的集合（也就是函数，可以有多种函数）
模型可以是条件概率分布函数或决策函数，具体为P(y|x) 或者 y=f(x)

# 统计学习三大要素
模型：
模型是输入空间到输出空间映射的集合，这个集合就是假设空间
模型分为概率模型和决策模型
=
策略：
从模型的假设空间中选择最优模型的准则
最优模型的评价标准是经验风险最小化和结构风险最小化

1 经验风险最小化：经验风险函数
举例：当模型是条件概率分布，损失函数是对数损失函数，最小经验风险就等价于极大似然估计
2 结构风险最小化：结构风险函数
是什么：
训练出来的模型对新数据的预测能力最优
结构风险最小化的实现是正则化，在经验风险最小化函数后面加上正则项，也称惩罚项，防止数据过拟合
为什么：
当数据量过小时，训练出来的模型可能会过拟合，也就是对新数据预测能力低
举例：
模型是条件概率分布，损失函数是对数损失函数，模型的复杂度由模型的先验概率表示时，结构风险最小化等价于最大后验概率估计
===
# 感知机 perceptron
感知机模型：
机器学习-感知机 perceptron http://blog.csdn.net/dream_angel_z/article/details/48915561
感知机是最原始的二分分类模型，找到一个超平面，将特征空间分离为两部分
函数模型是 f(x) = sign(wx+b)：
sign(x) = 1 (x>=0)
sign(x) = -1 (x<=0)
学习策略：
误分类点距超平面距离和为损失函数
超平面是 wx+b=0 ：
超平面 http://blog.csdn.net/denghecsdn/article/details/77313758l
算法实现：
随机梯度下降
如何理解感知机学习算法的对偶形式 https://www.zhihu.com/question/26526858

# K近邻法  k-Nearest Neighbor
K近邻法模型：
基于距离的分类算法，划分特征空间
输入新的x，在训练集中找出与x最临近的K个点，按照大多数投票表决x属于哪个分类
邻域：以a为开区间的区间，如（a-1.a+1）
距离：使用欧式距离作为与x距离
函数模型：
y = argmaxΣI(y_i = c_i)
I为指示函数 当y_i = c_i时为1,否则为0
?k值的选择:
策略：
经验风险函数：
误分类率为：1 -1/k*ΣI(y<i> = c<i>)
k临近模型的实现：
是什么：kd树，用超平面划分矩阵
为什么：提高效率，相比于计算每个点的距离

# 朴素贝叶斯法 native bayes
基于贝叶斯定理和特征条件独立假设，比较常见的分类算法
步骤：先算出p(x,y)
模型：
假设数据独立同分布，由联合概率p(x,y)产生，通过先验概率求后验概率
朴素贝叶斯方法（Naive Bayes）原理和实现 http://blog.csdn.net/tanhongguang1/article/details/45016421
条件独立性假设：
用于分类的特征在类确定的情况下都是独立的
就是为了简化计算，假设当前节点只与他的直接前序节点概率相关
贝叶斯网络中条件独立性是否是一种假设 https://www.zhihu.com/question/43481146/answer/95774249

函数模型：
先验 p(Y=c_k)
后验 p(X=x|Y=c_k)
y = f(x) = argmaxp(X=x|Y=c_k)

策略：
后验概率最大化等于期望风险最小化

算法：
1 极大似然估计
2 贝叶斯估计：
拉普拉斯平滑法，防止需要估计的概率值为0的情况

# 决策树
主要用于分类的模型，目的是为了建立分类的规则。输入x，通过特征不断判断x，最后归到相应的类
内部节点表示一个属性或者特征，叶节点表示类

决策树是if-then规则的集合
决策树也可表示给定特征条件下的条件概率分布：
特征空间划分 - 条件概率判断 - 决策树

决策树学习的步骤：
特征选择 - 决策树生成 - 剪枝
剪枝是为了防止过拟合

策略：
信息增益：
熵是什么：
判断 随机变量不确定是哪一个确定值的程度
熵越大，不确定性越大
经验熵和条件经验熵：H(D) H(D|A)
信息增益是什么：
已知特征X的信息对类Y不确定性减少的程度
为什么引入熵和信息增益：
选择最合适的特征
信息增益越大，特征A对数据集D分类的确定性增加的程度越大，也可以说对数据集不确定性减少的程度
g(D,A) = H(D) - H(D|A)
H(D|A) 越小，g(D,A)越大

# 逻辑斯蒂回归
问题：
f(-x+u)-1/2 = -f(x-u)+1/2
中心对称：图形围绕某点旋转180度能够重合
logistic 分布
位置参数 形状参数：
相当于μ，σ


===
数据挖掘——各种分类算法的优缺点 http://blog.csdn.net/shuke1991/article/details/52056382
数据挖掘是一门技能
熵权法简介 http://blog.sina.com.cn/s/blog_710e9b550101aqnv.html
熵权法的计算方法 https://zhuanlan.zhihu.com/p/28067337
NumPy和Pandas工具包中的函数 http://blog.csdn.net/baoyan2015/article/details/53503073
是什么：
从大量数据中挖掘出隐含的，未知的，对决策有潜在价值的关系、模式和趋势，并用这些知识建立用于决策支持的模型，提供预测性决策支持的方法，工具和过程
利用各种分析工具在大量数据中寻找其规律和发现模型与数据之间关系的过程，是统计学，数据库技术和人工智能技术的综合
为什么：
避免人治的随意性，避免企业管理仅仅依赖个人领导力的风险和不确定性，实现精细化营销和经营管理
     
数据挖掘的基本流程：
明确挖掘的目标 - 数据探索 - 数据预处理 - 建立模型 - 模型评价和应用

常用工具
anaconda
powerbi
superset数据分析平台
echart
===

# 明确挖掘目标
业务目标确定，模型确定，业务指标确定
     
# 数据探索
是什么：
检查数据质量，了解数据的规律
分为 数据质量分析 和 数据特征分析
为什么：
数据的准确性是数据挖掘的前提
数据特征分析能够有助于发现合适的建模指标

1 数据质量分析：
分为缺失值分析，异常值分析，重复数据、无关数据及特殊符号

2 数据特征分析：
基本统计量分析和分布分析，相关性分析和对比分析，帕累托分析和周期性分析

# 数据预处理
是什么：
清洗数据，选择建模指标，数据规范化和数据降维
为什么：
保证数据完整和准确，适应建模要求，提高数据建模的效率

数据规范化：
数据标准化处理，连续属性离散化，小波变换等
标准差标准化的作用。标准化分为指标趋同化和量纲归一化。指标趋同化的目的是保证数据的性质指向一致，常用方法是加符号或求倒数。
量纲归一化的目的是使数据收敛在一个范围，便于不同单位或量级的指标能够进行比较和加权
https://www.cnblogs.com/zhizhan/p/4676085.html
http://blog.sina.com.cn/s/blog_d8f8fbd40102vkmh.html

数据降维：
主成分降维:
用新的特征代替旧的特征，取得更少的特征向量

# 常用挖掘类型
1 分类与预测：
01 logistic 回归分类
代码实例：
logistic回归 https://github.com/parther/Python/blob/master/logistic%20%E5%9B%9E%E5%BD%92
灰色预测 https://github.com/parther/Python/blob/master/%E7%81%B0%E8%89%B2%E9%A2%84%E6%B5%8B

02 决策树
代码实例：
https://github.com/parther/Python/blob/master/%E5%AE%9E%E4%BE%8B%20%E5%86%B3%E7%AD%96%E6%A0%91

03 人工神经网络
代码实例：
https://github.com/parther/Python/blob/master/%E9%A2%84%E6%B5%8By%E5%80%BC
https://github.com/parther/Python/blob/master/%E9%A2%84%E6%B5%8By%E5%80%BC

04 时序模式
ARIMA 算法介绍：https://github.com/parther/Python/blob/master/%E6%97%B6%E9%97%B4%E5%BA%8F%E5%88%97
代码实例：
http://dwz.cn/6VM55r

2 聚类分析：
是什么：
在没有给定分类的情况下，按照数据的相似度进行样本分组的方法
分类模型需要使用有类别标记构成的训练数据，聚类模型可以建立在无类标记的数据上，是一种非监督的学习算法
作用：
细分领域特别有效；可以将连续数据离散
代码实例：
http://dwz.cn/6VM6DW
http://dwz.cn/6VM7i2

3 关联规则：
https://github.com/parther/Python/blob/master/%E5%85%B3%E8%81%94%E8%A7%84%E5%88%99
代码实例：
https://github.com/parther/Python/blob/master/%E7%AC%AC%E5%85%AB%E7%AB%A0%20%E5%85%B3%E8%81%94

4 离群点检测：
是什么：
检测数据集中的异常数据。主要应用于信用、安全领域
代码实例：

===
# 主成分降维：
f = '数据源/图书配套数据、代码/chapter4/demo/data/principal_component.xls'
d = pd.read_excel(f, header=None)

import pandas as pd
from sklearn.decomposition import PCA,FactorAnalysis
pca = PCA()
pca.fit(d)
pca.explained_variance_ratio_  # 查看贡献率

pca2=PCA(n_components=2) # 取前两个特征向量
reduce_X = pca2.fit_transform(d) # 数据降维 
reduce_X # 降维后的数据
