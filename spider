经验：
看官方文档
对比找出问题
如无必要，不要增加。剃刀原理。必要是没有太大的错误，没必要推到重来
每周五下午4点，总结此表
下午5点，解决问题；梳理做事的流程
为什么要加快效率呢？留出时间处理未知的bug，接受新的任务
一切从实际的项目入手
吃透问题，因为你肯定还会遇到这个问题。下午5点是复盘时间
从返回命令中找失败的原因，因为这个更准确
注重效率，不用怕犯错，因为害怕也没用
技术产生进步，采用一切能提高效率的方法
GitHub 搜索代码
采用复制 而不是剪切

===
# 问题：
导入的表格少数据 5060943 602781 987660 341744 6767600 无法导入mysql
将csv数据导入已存在的table没法成功
代码太长换行 跳出 EOL while scanning string literal 错误
导入到workbench的 user_list 修改字段last_login_time类型为datetime，数据无法读出
导入完csv文件user_list后提示 unhandled exception ascii codec cant encode character in position 36-37
导入的csv文件(原本为gbk转为utf-8格式)user_list 字段名user_id 变成了user id 并且无法查询 将文件转为utf无bom格式
关掉mysqlworkbench 的 SQL additions 板块
A value is trying to be set on a copy of a slice from a DataFrame.
存成csv文件多出1列
=
# 爬虫：
定义：用自动化的方式，从网站批量获取公开数据的技术
两个步骤：
1 获取源代码。网页是html文档，通过HTTP协议，客户机向服务器发送请求，获得html文档
2 解析源代码。爬虫的目的是获得从html文档，并从文档中挑选自己想要的东西
=
# 其他
df_post2.to_csv('df_post2.csv',index=False) 避免生出第一列
import pandas as pd  
from sqlalchemy import create_engine
engine = create_engine('mysql+pymysql://root:12345@127.0.0.1:3306/tianya_scrapy?charset=utf8')  
df_post.to_sql(name='df_post', con=engine, if_exists='append',index=False,index_label=False) 

=
# MySQL
Error Code: 1235. This version of MySQL doesn't yet support 'LIMIT & IN/ALL/ANY/SOME subquery'
delete from user_list where user_id in (select t.user_id from (select * from user_list limit 1,570) as t )
https://www.cnblogs.com/azhqiang/p/5942093.html
-
Error Code: 1175. You are using safe update mode and you tried to update a table without a 
WHERE that uses a KEY column To disable safe mode, toggle the option in Preferences -> SQL Editor and reconnect.
SET SQL_SAFE_UPDATES = 0
https://jingyan.baidu.com/article/e5c39bf58ed69239d76033a4.html
把Preference中的标签由General切换到SQL Queries标签，然后支去掉safe updates前的选项框
pymysql.err.interfaceerror # self.conn.close() 断开数据库连接 https://stackoverflow.com/questions/6650940/interfaceerror-0
mysql导入user_list 需要将所有数据类型改为text才能保证导入不被去空
导入数据多列变成一列 设置seperate
MySQL server version for the right syntax to use near 就是语法错误，不管是否返回正确结果
=
# scrapy 概述
Scrapy是一个为了爬取网站数据和提取结构性数据而编写的应用框架，可以省去造轮子的时间
Python爬虫进阶一之爬虫框架概述： 
https://cuiqingcai.com/2433.html
https://www.cnblogs.com/jinxiao-pu/p/6762636.html
入门教程： 
https://doc.scrapy.org/en/1.5/topics/commands.html
中文版 http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/overview.html
Scrapy中的Rules理解 http://blog.csdn.net/wqh_jingsong/article/details/56865433
提升scrapy的性能 https://segmentfault.com/a/1190000009321902
? start_request的用法
item文件中的条目可以全列出来，不会影响spider文件中的操作，少了却是不行
response.url 取得从上级拿下来的url
response.url 是 str格式
item['label'] 为str
=

# scrapy报错解决
unknown command : crawl :指定到文件夹路径
list out of range 条目不对
must be real number, not str: 一般是正则表达的问题
''' 与下行的缩进保持一致才不会报错
多页面多条目二级爬取，部分结果无法爬出，返回503错误：
因为服务器过载导致503，请降低爬虫的并发访问数量，并且延长各个请求之间的间隔时间 https://www.zhihu.com/question/34505058 
itemmeta doesnot support item assign 没加括号
feedexporter object has no attribute slot 导出时文件关闭
返回状态200却没有抓取到内容，是xpath问题或者没有添加yield item
JSONDecodeError 一般是json链接的格式不对，删除var

-
解决空行的问题 找到anaconda路径，修改源码 http://www.mamicode.com/info-detail-2000839.html
? 固定爬取的顺序 爬下来字段的顺序是乱的 https://www.jianshu.com/p/fd6f7eba6abe
获取失败之后，重新将url放入待抓取队列，设置request的重试次数 https://segmentfault.com/q/1010000006899112/a-1020000006901254
存储路径的两种写法：
FEED_URI = u'file:///C:/Users/Administrator/Desktop/douban.csv'
FEED_URI = './xf_info.csv'
ITEM_PIPELINES = {'beta.pipelines.BetaPipeline': 300,} 启用pipe存储
用for语句存储item才能保证爬取的字符在csv中换行显示
更新数据库的方法 http://blog.csdn.net/ljm_9615/article/details/76715696
post 94个链接，只出来63个，出现404 HTTP status code is not handled or allowed ，单个抓取不会
HTTPERROR_ALLOWED_CODES = [404] http://blog.csdn.net/tingfenyijiu/article/details/78009755
=
# 从案例学习scrapy
爬虫教程 https://cuiqingcai.com/1052.html
爬虫教程 http://blog.csdn.net/column/details/python3-spider.html
使用LinkExtractor爬取二级页面 http://blog.csdn.net/u010814042/article/details/74127309
写入数据库；翻页 http://blog.csdn.net/Gavin_CHEN929/article/details/56509735
翻页 http://blog.csdn.net/dylanzr/article/details/51764694
优秀代码 https://chenjiabing666.github.io/categories/Scrapy%E5%AD%A6%E4%B9%A0/

=
# scrapy 二级爬取
使用meta的方法 https://www.jianshu.com/p/fe8a75284f9b 此案例的二级页面结果重复
=
# scrapy 翻页
for 结合 start_urls
? phase 页面内提交自己解析
=
# xpath：
? 用xpath 去除所选元素的空格 normalize-space http://blog.csdn.net/zhouxuan623/article/details/51241784
xpath官方文档 ttp://www.w3school.com.cn/xpath
xpath定位 https://www.cnblogs.com/xxyBlogs/p/4244073.html
Selenium beautifulsoup的用法 http://cuiqingcai.com/1052.html
-
使用Chrome获得xpath 鼠标移动到指定文本-审查元素-copy-copy xpath
将多个要爬取的xpath拿出来比较能够更快看出规律
xpath的另一种方式 '//div[@class="bbs-content"]/text()' 
//div[@class="portrait"]/h2/a[1]/text() @class="portrait"一定要双引号 
Selector和response的用法没有差别，可互相替换
response.xpath 返回的是list数据
xpath爬取a标签所有字符 https://www.tuicool.com/articles/iqQFBn response.xpath('//*[@id="bd"]').xpath('string(.)').extract()
选取含某属性的标签内容 //div[contains(@class, 'demo')] https://segmentfault.com/q/1010000000779093 v
选取同级元素 response.xpath('//*[@class="user-location"]/following-sibling::text()').extract()
=
# scrapy 爬虫代理
不使用框架开代理 http://blog.csdn.net/loguat/article/details/74740229
国内高匿代理IP http://www.xicidaili.com/nn/2640
利用crawlera神器，无需再寻找代理IP http://blog.csdn.net/xiao4816/article/details/50650075
设置ip代理和user-agent：
随机useragent 和 ip https://www.cnblogs.com/jinxiao-pu/p/6762636.html
useragent 和 ip https://www.cnblogs.com/jinxiao-pu/p/6665180.html
useragent 和 ip 2  https://www.cnblogs.com/xiaomingzaixian/p/7125796.html
自动更新IP池 http://blog.csdn.net/u011781521/article/details/70194744
洋葱代理的使用方法 https://www.cnblogs.com/qiyeboy/p/5577036.html
使用的ip必须开代理，必须加端口
API提取
使用xici代理 http://blog.csdn.net/fighting_one_piece/article/details/52961412

# 关于js
方法有二：手动获取；模拟浏览器获取
-
Selenium：
Python爬虫利器五之Selenium的用法 https://cuiqingcai.com/2599.html
Scrapy+PhantomJS+Selenium动态爬虫 http://blog.csdn.net/qq_30242609/article/details/70859891
Scrapy+phantomjs爬取动态网页数据 http://blog.csdn.net/u010085423/article/details/54944502
Selenium with Python中文翻译文档 http://selenium-python-zh.readthedocs.io/en/latest/index.html
Selenium与PhantomJS操作 https://www.cnblogs.com/mayi0312/p/7231242.html
实例： http://blog.csdn.net/kangqianglong/article/details/62045982
实例：python 爬虫如何获取js里面的内容 http://blog.csdn.net/hanchaobiao/article/details/73150405
实例：http://blog.csdn.net/And_w/article/details/73611325
设置PHANTOMJS的USER-AGENT http://smilejay.com/2013/12/set-user-agent-for-phantomjs/
trs = browser.find_element_by_xpath('//*[@id="proxies_table"]/tbody/tr[1]/td[1]')
trs.text.split(' ') trs为str
-
splash：
利用scrapy-splash爬取JS生成的动态页面 https://www.cnblogs.com/zhonghuasong/p/5976003.html
实例：利用scrapy-splash爬取腾讯的证券新闻 https://www.mylonly.com/15052000291680.html
实例：scrapy-splash抓取动态数据例子一 https://www.cnblogs.com/shaosks/p/6950358.html
实例：scrapy-splash抓取动态数据例子二 https://www.cnblogs.com/shaosks/p/6961951.html
实例：https://edu.hellobi.com/course/156/lessons
windows下安装docker http://blog.csdn.net/tina_ttl/article/details/51372604

Docker安装：
Docker Toolbox https://docs.docker.com/toolbox/toolbox_install_windows/
Windows10 使用docker toolbox安装docker http://www.cnblogs.com/shaosks/p/6932319.html
IP 获取不到 github https://github.com/docker/toolbox/issues/317：
$ docker-machine rm default
$ docker-machine create --driver virtualbox default
本地查看渲染情况 http://192.168.99.100:8050/
-
Python爬虫:更加优雅的执行JavaScript(PyV8) https://www.jianshu.com/p/c534d6eb881a?utm_source=oschina-app
=
# 解析json
XHR-copy link-address
解析json页面 https://www.cnblogs.com/voidsky/p/5490786.html
先使用简单的方法
先按照自己的思路来，再看参考
关于js的问题还是没搞清楚就开始做了，js和json不一样
fiddler


