# 爬虫
定义：用自动化的方式，从网站批量获取公开数据的技术
工作流程：
1 获取源代码。网页是html文档，通过HTTP协议，客户机向服务器发送请求，获得html文档
2 解析源代码。爬虫的目的是获得从html文档，并从文档中挑选自己想要的东西

# scrapy
定义：Scrapy是一个爬虫框架（相当于模板），能够减少工作量，省去造轮子的时间
工作流程：发送请求-获得回应并解析-将解析内容交由管道存储
-
参考：
官方文档 https://doc.scrapy.org/en/1.5/topics/commands.html
官方文档-中文版 http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/overview.html
框架说明 https://cuiqingcai.com/2433.html
框架说明-工作流程图 https://www.cnblogs.com/jinxiao-pu/p/6762636.html

提升scrapy的性能 https://segmentfault.com/a/1190000009321902
定时爬虫 http://blog.csdn.net/vivian_ll/article/details/65442105

爬虫教程 https://cuiqingcai.com/1052.html
爬虫教程 http://blog.csdn.net/column/details/python3-spider.html
优秀代码 https://chenjiabing666.github.io/categories/Scrapy%E5%AD%A6%E4%B9%A0/



item文件中的条目可以全列出来，不会影响spider文件中的操作，少了却是不行
response.url 取得从上级拿下来的url，是 str格式
item['label'] 为str

返回503错误：
因为服务器过载导致503，请降低爬虫的并发访问数量，并且延长各个请求之间的间隔时间
设置允许503错误
404错误 HTTP status code is not handled or allowed。
HTTPERROR_ALLOWED_CODES = [404] http://blog.csdn.net/tingfenyijiu/article/details/78009755
设置request重试 https://segmentfault.com/q/1010000006899112/a-1020000006901254
itemmeta doesnot support item assign。没加括号item()
feedexporter object has no attribute slot。导出文件未关闭
JSONDecodeError。 一般是json链接的格式不对，可能含var需删除

-
导出到csv表中每插入一行数据空一行。找到anaconda路径，修改源码 http://www.mamicode.com/info-detail-2000839.html
导出到csv字段的顺序与items中规定的不一样。https://www.jianshu.com/p/fd6f7eba6abe


存储到csv文件路径的两种写法：
FEED_URI = u'file:///C:/Users/Administrator/Desktop/douban.csv'
FEED_URI = './xf_info.csv'

启用pipe存储。ITEM_PIPELINES = {'beta.pipelines.BetaPipeline': 300,} 
将scrapy爬取的数据存入数据库。http://blog.csdn.net/ljm_9615/article/details/76715696




使用scrapy爬取二级页面，但不推荐：
使用yield scrapy.Request https://www.jianshu.com/p/fe8a75284f9b
使用LinkExtractor爬取二级页面 http://blog.csdn.net/u010814042/article/details/74127309
Scrapy中的Rules理解 http://blog.csdn.net/wqh_jingsong/article/details/56865433
使用meta存储一级页面和二级页面抓取的结果 https://www.jianshu.com/p/fe8a75284f9b

使用callback=self.parse 翻页 http://blog.csdn.net/dylanzr/article/details/51764694

=
# xpath：
xpath官方文档 http://www.w3school.com.cn/xpath
xpath定位 https://www.cnblogs.com/xxyBlogs/p/4244073.html
获得xpath路径 Chrome内核浏览器-鼠标移动到指定文本-审查元素-copy-copy xpath
获得xpath规律 多个xpath比较
获得属性值 '//div[@class="bbs-content"]/a/@href' 
获得标签下文本 '//div[@class="bbs-content"]/a/text()' 
获取标签下的所有文本 response.xpath('//*[@class="user-location"]/following-sibling::text()').extract()
@class="portrait"需为双引号 //div[@class="portrait"]/h2/a[1]/text() 
Selector和response的用法没有差别，可互相替换
response.xpath 返回的是list数据
获取标签下所有字符。response.xpath('//*[@id="bd"]').xpath('string(.)').extract()
获取含某属性的标签的内容。 //div[contains(@class, 'demo')]

=
# scrapy 爬虫代理
不使用框架开代理 http://blog.csdn.net/loguat/article/details/74740229
国内高匿代理IP http://www.xicidaili.com/nn/2640
利用crawlera神器，无需再寻找代理IP http://blog.csdn.net/xiao4816/article/details/50650075
设置ip代理和user-agent：
随机useragent 和 ip https://www.cnblogs.com/jinxiao-pu/p/6762636.html
useragent 和 ip https://www.cnblogs.com/jinxiao-pu/p/6665180.html
useragent 和 ip 2  https://www.cnblogs.com/xiaomingzaixian/p/7125796.html
自动更新IP池 http://blog.csdn.net/u011781521/article/details/70194744
洋葱代理的使用方法 https://www.cnblogs.com/qiyeboy/p/5577036.html
使用的ip必须开代理，必须加端口
API提取
使用xici代理 http://blog.csdn.net/fighting_one_piece/article/details/52961412

# 关于js
方法有二：手动获取；模拟浏览器获取
-
Selenium beautifulsoup的用法 http://cuiqingcai.com/1052.html
Selenium：
Python爬虫利器五之Selenium的用法 https://cuiqingcai.com/2599.html
Scrapy+PhantomJS+Selenium动态爬虫 http://blog.csdn.net/qq_30242609/article/details/70859891
Scrapy+phantomjs爬取动态网页数据 http://blog.csdn.net/u010085423/article/details/54944502
Selenium with Python中文翻译文档 http://selenium-python-zh.readthedocs.io/en/latest/index.html
Selenium与PhantomJS操作 https://www.cnblogs.com/mayi0312/p/7231242.html
实例： http://blog.csdn.net/kangqianglong/article/details/62045982
实例：python 爬虫如何获取js里面的内容 http://blog.csdn.net/hanchaobiao/article/details/73150405
实例：http://blog.csdn.net/And_w/article/details/73611325
设置PHANTOMJS的USER-AGENT http://smilejay.com/2013/12/set-user-agent-for-phantomjs/
trs = browser.find_element_by_xpath('//*[@id="proxies_table"]/tbody/tr[1]/td[1]')
trs.text.split(' ') trs为str
-
splash：
利用scrapy-splash爬取JS生成的动态页面 https://www.cnblogs.com/zhonghuasong/p/5976003.html
实例：利用scrapy-splash爬取腾讯的证券新闻 https://www.mylonly.com/15052000291680.html
实例：scrapy-splash抓取动态数据例子一 https://www.cnblogs.com/shaosks/p/6950358.html
实例：scrapy-splash抓取动态数据例子二 https://www.cnblogs.com/shaosks/p/6961951.html
实例：https://edu.hellobi.com/course/156/lessons
windows下安装docker http://blog.csdn.net/tina_ttl/article/details/51372604

Docker安装：
Docker Toolbox https://docs.docker.com/toolbox/toolbox_install_windows/
Windows10 使用docker toolbox安装docker http://www.cnblogs.com/shaosks/p/6932319.html
IP 获取不到 github https://github.com/docker/toolbox/issues/317：
$ docker-machine rm default
$ docker-machine create --driver virtualbox default
本地查看渲染情况 http://192.168.99.100:8050/
-
Python爬虫:更加优雅的执行JavaScript(PyV8) https://www.jianshu.com/p/c534d6eb881a?utm_source=oschina-app
=
# 解析json
XHR-copy link-address
解析json页面 https://www.cnblogs.com/voidsky/p/5490786.html
先使用简单的方法
先按照自己的思路来，再看参考
关于js的问题还是没搞清楚就开始做了，js和json不一样
fiddler
换页有两种方法 xpath和根据页面的规则

