《Python爬虫开发与项目实战》.pdf

原始内容在 response.content 里，bytes

# 爬虫
定义：用自动化的方式，从网站/app批量获取公开数据的技术
工作流程：
1 获取源代码。网页是html文档，通过HTTP协议，客户机向服务器发送请求，获得html文档
2 解析源代码。爬虫的目的是获得从html文档，并从文档中挑选自己想要的东西
建议：
爬虫爬取和爬虫解析是独立的，爬虫解析的数据源必须来自于爬虫爬取的原始数据源
爬虫最好都需要加上标志性的时间或者是createtime
数据的更新操作都是放在最后的
=
# scrapy
定义：Scrapy是一个爬虫框架（相当于模板），能够减少工作量，省去造轮子的时间
工作流程：发送请求-获得回应并解析-将解析内容交由管道存储
-
# 参考：
官方文档 https://doc.scrapy.org/en/1.5/topics/commands.html
官方文档-中文版 http://scrapy-chs.readthedocs.io/zh_CN/1.0/intro/overview.html
框架说明 https://cuiqingcai.com/2433.html
框架说明-工作流程图 https://www.cnblogs.com/jinxiao-pu/p/6762636.html
-
提升scrapy的性能 https://segmentfault.com/a/1190000009321902
定时爬虫 http://blog.csdn.net/vivian_ll/article/details/65442105
-
爬虫教程 https://cuiqingcai.com/1052.html
爬虫教程 http://blog.csdn.net/column/details/python3-spider.html
优秀代码 https://chenjiabing666.github.io/categories/Scrapy%E5%AD%A6%E4%B9%A0/

=
# 基础
item文件中的条目可以全列出来，不会影响spider文件中的操作，少了却是不行
response.url 取得从上级拿下来的url，是 str格式
item['label'] 为str


=
# 错误处理
返回503错误：
因为服务器过载导致503，请降低爬虫的并发访问数量，并且延长各个请求之间的间隔时间
设置允许503错误
404错误 HTTP status code is not handled or allowed。
HTTPERROR_ALLOWED_CODES = [404] http://blog.csdn.net/tingfenyijiu/article/details/78009755
设置request重试 https://segmentfault.com/q/1010000006899112/a-1020000006901254
itemmeta doesnot support item assign。没加括号item()
feedexporter object has no attribute slot。导出文件未关闭
JSONDecodeError。 一般是json链接的格式不对，可能含var需删除
start_urls = ['url'] 链接不能存在空格' '
检查scrapy运行的结果，先看请求是否响应，也就是response state
scrapy 查看错误日志
https://blog.csdn.net/hopygreat/article/details/78771500

=
# 存储
导出到csv表中每插入一行数据空一行。找到anaconda路径，修改源码 http://www.mamicode.com/info-detail-2000839.html
导出到csv字段的顺序与items中规定的不一样。https://www.jianshu.com/p/fd6f7eba6abe
存储到csv文件路径的两种写法：
FEED_URI = u'file:///C:/Users/Administrator/Desktop/douban.csv'
FEED_URI = './xf_info.csv'
-
启用pipe存储。ITEM_PIPELINES = {'beta.pipelines.BetaPipeline': 300,} 
将scrapy爬取的数据存入数据库。http://blog.csdn.net/ljm_9615/article/details/76715696

=
# 多级页面处理
使用scrapy爬取二级页面，但不推荐：
使用yield scrapy.Request https://www.jianshu.com/p/fe8a75284f9b
使用LinkExtractor爬取二级页面 http://blog.csdn.net/u010814042/article/details/74127309
Scrapy中的Rules理解 http://blog.csdn.net/wqh_jingsong/article/details/56865433
使用meta存储一级页面和二级页面抓取的结果 https://www.jianshu.com/p/fe8a75284f9b
-
使用callback=self.parse 翻页 http://blog.csdn.net/dylanzr/article/details/51764694

=
# xpath：
xpath官方文档 http://www.w3school.com.cn/xpath
xpath定位 https://www.cnblogs.com/xxyBlogs/p/4244073.html
获得xpath路径 Chrome内核浏览器-鼠标移动到指定文本-审查元素-copy-copy xpath
获得xpath规律 多个xpath比较
获得属性值 '//div[@class="bbs-content"]/a/@href' 
获得标签下文本 '//div[@class="bbs-content"]/a/text()' 
获取标签下的所有文本 response.xpath('//*[@class="user-location"]/following-sibling::text()').extract()
@class="portrait"需为双引号 //div[@class="portrait"]/h2/a[1]/text() 
Selector和response的用法没有差别，可互相替换
response.xpath 返回的是list数据
获取标签下所有字符。response.xpath('//*[@id="bd"]').xpath('string(.)').extract()
获取含某属性的标签的内容。 //div[contains(@class, 'demo')]
依靠文本名字定位 response.xpath('/html/body/ol/li[text()="[发文字号]"]/span/text()').extract())

=
# scrapy 爬虫代理
代理ip需添加端口 
? 代理API提取
xici代理http://www.xicidaili.com/nn/2640
自动更新xici代理IP池1 http://blog.csdn.net/u011781521/article/details/70194744
自动更新xici代理IP池2 http://blog.csdn.net/fighting_one_piece/article/details/52961412
洋葱代理 https://www.cnblogs.com/qiyeboy/p/5577036.html
使用crawlera代理 http://blog.csdn.net/xiao4816/article/details/50650075
-
设置useragent和ip代理 https://www.cnblogs.com/jinxiao-pu/p/6665180.html
不使用scrapy开代理 http://blog.csdn.net/loguat/article/details/74740229

=
# 获取网页动态数据
1 抓包
抓取json包 https://blog.csdn.net/hanchaobiao/article/details/73150405
scrapy爬虫与动态页面——爬取拉勾网职位信息（1） https://www.cnblogs.com/voidsky/p/5490786.html
2 Selenium
Selenium with Python官方文档 http://selenium-python-zh.readthedocs.io/en/latest/index.html
Selenium beautifulsoup的用法 http://cuiqingcai.com/1052.html
Selenium是自动化测试工具 https://cuiqingcai.com/2599.html
Selenium与无界面浏览器 PhantomJS https://www.cnblogs.com/mayi0312/p/7231242.html
设置PHANTOMJS的USER-AGENT。http://smilejay.com/2013/12/set-user-agent-for-phantomjs/
使用 Selenium爬取动态网页数据 http://blog.csdn.net/And_w/article/details/73611325
使用 Selenium+Chrome/PhantomJS抓取淘宝美食 https://edu.hellobi.com/course/156/lessons
-
Scrapy+phantomjs爬取动态网页数据 http://blog.csdn.net/u010085423/article/details/54944502
Scrapy+phantomjs爬取动态网页数据http://blog.csdn.net/kangqianglong/article/details/62045982
3 使用splash
利用scrapy-splash爬取JS生成的动态页面 https://www.cnblogs.com/zhonghuasong/p/5976003.html
利用scrapy-splash爬取腾讯的证券新闻 https://www.mylonly.com/15052000291680.html
scrapy-splash抓取动态数据例子一 https://www.cnblogs.com/shaosks/p/6950358.html
scrapy-splash抓取动态数据例子二 https://www.cnblogs.com/shaosks/p/6961951.html
-
windows下安装docker http://blog.csdn.net/tina_ttl/article/details/51372604
Docker安装 Docker Toolbox https://docs.docker.com/toolbox/toolbox_install_windows/
Windows10 使用docker toolbox安装docker http://www.cnblogs.com/shaosks/p/6932319.html
IP 获取不到 github https://github.com/docker/toolbox/issues/317：
$ docker-machine rm default
$ docker-machine create --driver virtualbox default
本地查看渲染情况 http://192.168.99.100:8050/
4
Python爬虫:更加优雅的执行JavaScript(PyV8) https://www.jianshu.com/p/c534d6eb881a?utm_source=oschina-app

=
常规爬虫代码，使用requests ：
import requests  
from lxml import etree
headers = {'User-Agent':'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36'}
res = requests.get('http://search.tianya.cn/user?q=%E4%B8%8A%E8%AE%BF&pn=1')  
res.encoding = 'utf-8'
user_id = etree.HTML(res.text).xpath('//*[@id="main"]/div[2]/ul/li/p[1]/a/@href')

bs语法：
import requests
from bs4 import BeautifulSoup
html = requests.get('http://www.pm25.com/xian.html')
soup = BeautifulSoup(html.text,'html.parser')
https://cuiqingcai.com/1319.html

